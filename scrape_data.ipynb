{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15eda0e7",
   "metadata": {},
   "source": [
    "# Introduction to the Data Scraping Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87381048",
   "metadata": {},
   "source": [
    "In this data scraping notebook, our objective is to gather relevant information from two key sources – The White House and The European Commission. By systematically collecting and processing data, we aim to provide valuable insights into the nature of their support, potential differences in rhetoric, and the impact of President Zelenskiy's visits.\n",
    "\n",
    "The notebook includes pipelines for scraping the data from The White House and The European Commission. The scraping classes are implemented in a scr/scraper.py file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f728c5d7",
   "metadata": {},
   "source": [
    "# Set Up Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b78d4a1",
   "metadata": {},
   "source": [
    "## Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2268afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import src.scraper as s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520acff8",
   "metadata": {},
   "source": [
    "# Scrape The White House Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66de53e2",
   "metadata": {},
   "source": [
    "In this section, we retrieve data from The White House, focusing on President Biden's administration. This includes parsing official statements, speeches, and and press briefings. \n",
    "\n",
    "The data from The White House will be crucial in understanding the United States' stance and support for Ukraine, particularly in the context of President Zelenskiy's visits. It forms a foundational component of our comparative analysis with The European Commission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d68d9",
   "metadata": {},
   "source": [
    "## Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05d9dcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speeches-remarks\n",
      "Total number of pages: 202\n",
      "20/202 completed.\n",
      "40/202 completed.\n",
      "60/202 completed.\n",
      "80/202 completed.\n",
      "100/202 completed.\n",
      "120/202 completed.\n",
      "140/202 completed.\n",
      "160/202 completed.\n",
      "180/202 completed.\n",
      "200/202 completed.\n",
      "\n",
      "statements-releases\n",
      "Total number of pages: 512\n",
      "51/512 completed.\n",
      "102/512 completed.\n",
      "153/512 completed.\n",
      "204/512 completed.\n",
      "255/512 completed.\n",
      "306/512 completed.\n",
      "357/512 completed.\n",
      "408/512 completed.\n",
      "459/512 completed.\n",
      "510/512 completed.\n",
      "\n",
      "press-briefings\n",
      "Total number of pages: 92\n",
      "9/92 completed.\n",
      "18/92 completed.\n",
      "27/92 completed.\n",
      "36/92 completed.\n",
      "45/92 completed.\n",
      "54/92 completed.\n",
      "63/92 completed.\n",
      "72/92 completed.\n",
      "81/92 completed.\n",
      "90/92 completed.\n",
      "\n",
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dataframe to store the results\n",
    "wh_articles_df = pd.DataFrame(columns=['Title', 'Date', 'Category', 'Location', 'Text'])\n",
    "\n",
    "# List of categories to be scraped\n",
    "wh_links = ['https://www.whitehouse.gov/briefing-room/speeches-remarks/',\n",
    "            'https://www.whitehouse.gov/briefing-room/statements-releases/',\n",
    "            'https://www.whitehouse.gov/briefing-room/press-briefings/']        \n",
    "\n",
    "# Iterate through all categories\n",
    "for link in wh_links:\n",
    "    # Get the category\n",
    "    category = link.split('/')[-2]\n",
    "    print(category)\n",
    "    # Initialize the scraping class\n",
    "    scraper = s.TheWhiteHouseScraper(url=link)\n",
    "    soup = scraper.get_html_content()\n",
    "    \n",
    "    # Get the total number of pages\n",
    "    page_num = scraper.get_page_num(soup)\n",
    "    print(f'Total number of pages: {page_num}')\n",
    "\n",
    "    # Get articles from each page\n",
    "    for i in range(1, page_num+1):\n",
    "        page_link = f'{link}page/{i}/'\n",
    "        page_scraper = s.TheWhiteHouseScraper(url=page_link)\n",
    "        page_soup = page_scraper.get_html_content()\n",
    "        \n",
    "        # Add articles to a dataframe\n",
    "        df_temp = pd.DataFrame(page_scraper.get_articles(page_soup, category))\n",
    "        wh_articles_df = pd.concat([wh_articles_df, df_temp], ignore_index=True)\n",
    "\n",
    "        # Print progress every 10%\n",
    "        if i % (page_num // 10) == 0:\n",
    "            print(f'{i}/{page_num} completed.')\n",
    "    print()\n",
    "\n",
    "print(\"Scraping completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea7ec46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8053, 5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc86d778",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Date</th>\n",
       "      <th>Category</th>\n",
       "      <th>Location</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Remarks by President Biden and Vice President ...</td>\n",
       "      <td>2024-02-03T22:00:00-05:00</td>\n",
       "      <td>Speeches and Remarks</td>\n",
       "      <td>Biden for President Campaign Headquarters; Wil...</td>\n",
       "      <td>THE VICE PRESIDENT:  Hello, Delaware!  (Applau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Remarks by Vice President Harris at a Campaign...</td>\n",
       "      <td>2024-02-02T23:33:00-05:00</td>\n",
       "      <td>Speeches and Remarks</td>\n",
       "      <td>South Carolina State University; Orangeburg, S...</td>\n",
       "      <td>THE VICE PRESIDENT:  All right.  Can we hear i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Remarks by President Biden at a Political Even...</td>\n",
       "      <td>2024-02-01T20:24:19-05:00</td>\n",
       "      <td>Speeches and Remarks</td>\n",
       "      <td>Region 1 Union Hall; Warren, Michigan</td>\n",
       "      <td>4:41 P.M. EST\\n \\nTHE PRESIDENT:  Well, thank ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Remarks by President Biden at the National Pra...</td>\n",
       "      <td>2024-02-01T14:13:03-05:00</td>\n",
       "      <td>Speeches and Remarks</td>\n",
       "      <td>U.S. Capitol; Washington, D.C.</td>\n",
       "      <td>9:04 A.M. EST\\nTHE PRESIDENT:  Frank, thank yo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Remarks by President Biden at a Campaign Recep...</td>\n",
       "      <td>2024-01-31T00:04:32-05:00</td>\n",
       "      <td>Speeches and Remarks</td>\n",
       "      <td>Private Residence; Miami, Florida</td>\n",
       "      <td>6:27 P.M. EST\\n\\nTHE PRESIDENT: Well, Chris, t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0  Remarks by President Biden and Vice President ...   \n",
       "1  Remarks by Vice President Harris at a Campaign...   \n",
       "2  Remarks by President Biden at a Political Even...   \n",
       "3  Remarks by President Biden at the National Pra...   \n",
       "4  Remarks by President Biden at a Campaign Recep...   \n",
       "\n",
       "                        Date              Category  \\\n",
       "0  2024-02-03T22:00:00-05:00  Speeches and Remarks   \n",
       "1  2024-02-02T23:33:00-05:00  Speeches and Remarks   \n",
       "2  2024-02-01T20:24:19-05:00  Speeches and Remarks   \n",
       "3  2024-02-01T14:13:03-05:00  Speeches and Remarks   \n",
       "4  2024-01-31T00:04:32-05:00  Speeches and Remarks   \n",
       "\n",
       "                                            Location  \\\n",
       "0  Biden for President Campaign Headquarters; Wil...   \n",
       "1  South Carolina State University; Orangeburg, S...   \n",
       "2              Region 1 Union Hall; Warren, Michigan   \n",
       "3                     U.S. Capitol; Washington, D.C.   \n",
       "4                  Private Residence; Miami, Florida   \n",
       "\n",
       "                                                Text  \n",
       "0  THE VICE PRESIDENT:  Hello, Delaware!  (Applau...  \n",
       "1  THE VICE PRESIDENT:  All right.  Can we hear i...  \n",
       "2  4:41 P.M. EST\\n \\nTHE PRESIDENT:  Well, thank ...  \n",
       "3  9:04 A.M. EST\\nTHE PRESIDENT:  Frank, thank yo...  \n",
       "4  6:27 P.M. EST\\n\\nTHE PRESIDENT: Well, Chris, t...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c4dd459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "wh_articles_df.to_csv('data/thewhitehouse.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447d38f9",
   "metadata": {},
   "source": [
    "# Scrape The European Commission Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27429bb1",
   "metadata": {},
   "source": [
    "In this phase, our focus shifts to collecting data from The European Commission, which plays a significant role in the European Union's policies and actions.\n",
    "\n",
    "We specifically are going to focus on the speeches and remarks of Ursula von der Leyen, the current President of the European Commission. The data collected will offer insights into the European Commission's stance and support for Ukraine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d52d2956",
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://ec.europa.eu/commission/presscorner/home/en'\n",
    "geko_path = '/Users/viktoriia/Desktop/BSE/Term 2/NLP/NLP-Project/geckodriver'\n",
    "profile_path = '/Users/viktoriia/Library/Application Support/Firefox/Profiles/k7kr4dw0.Viktoriia'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8073d8be",
   "metadata": {},
   "source": [
    "## Scraping Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b3e9a62",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Iterate the pages \u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m scraper\u001b[38;5;241m.\u001b[39melement_exists(browser, path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//a[@title=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGo to next page\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, e_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;66;03m# fix this, because we are missing the data from the last page\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m     browser, temp_list \u001b[38;5;241m=\u001b[39m scraper\u001b[38;5;241m.\u001b[39mget_articles(browser)\n\u001b[1;32m     28\u001b[0m     df_temp \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(temp_list)\n\u001b[1;32m     29\u001b[0m     ec_articles_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([ec_articles_df, df_temp], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/BSE/Term 2/NLP/NLP-Project/src/scraper.py:223\u001b[0m, in \u001b[0;36mTheEuropeanCommissionScraper.get_articles\u001b[0;34m(self, browser)\u001b[0m\n\u001b[1;32m    221\u001b[0m browser\u001b[38;5;241m.\u001b[39mswitch_to\u001b[38;5;241m.\u001b[39mwindow(browser\u001b[38;5;241m.\u001b[39mwindow_handles[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    222\u001b[0m browser\u001b[38;5;241m.\u001b[39mget(a_link)\n\u001b[0;32m--> 223\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    225\u001b[0m a_text \u001b[38;5;241m=\u001b[39m browser\u001b[38;5;241m.\u001b[39mfind_element(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxpath\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m//div[@class=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mecl-paragraph-detail\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    227\u001b[0m temp_list\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTitle\u001b[39m\u001b[38;5;124m'\u001b[39m: a_title, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m: a_date,\n\u001b[1;32m    228\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCategory\u001b[39m\u001b[38;5;124m'\u001b[39m: a_category, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mText\u001b[39m\u001b[38;5;124m'\u001b[39m: a_text})\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize an empty dataframe to store the results\n",
    "ec_articles_df = pd.DataFrame(columns=['Title', 'Date', 'Category', 'Location', 'Text'])\n",
    "\n",
    "# Initialize type flag\n",
    "first_type = True\n",
    "\n",
    "# CSS for all types of documents we are interested in\n",
    "document_types_css = {'STATEMENT' : '#filter-documentType > option:nth-child(13)', \n",
    "                  'SPEECH': '#filter-documentType > option:nth-child(12)',\n",
    "                  'PRESS RELEASE': '#filter-documentType > option:nth-child(9)'}\n",
    "\n",
    "\n",
    "for document_type in ['STATEMENT', 'SPEECH', 'PRESS RELEASE']:\n",
    "    if first_type:\n",
    "        # Initialize the scraper and open the main page\n",
    "        scraper = s.TheEuropeanCommissionScraper()\n",
    "        browser = scraper.start_up(link, geko_path, profile_path)\n",
    "    else:\n",
    "        # Open the main page\n",
    "        browser = scraper.start_up(link, geko_path, profile_path, browser)\n",
    "\n",
    "    browser = scraper.fill_in_filters(browser, document_type = document_types_css[document_type])    \n",
    "    \n",
    "    # Iterate the pages \n",
    "    while scraper.element_exists(browser, path='//a[@title=\"Go to next page\"]', e_type='xpath'):\n",
    "        # fix this, because we are missing the data from the last page\n",
    "        browser, temp_list = scraper.get_articles(browser)\n",
    "        df_temp = pd.DataFrame(temp_list)\n",
    "        ec_articles_df = pd.concat([ec_articles_df, df_temp], ignore_index=True)\n",
    "        browser.find_element('xpath','//a[@title=\"Go to next page\"]').click()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    first_type = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c0f73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90057fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ac1bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "ec_articles_df.to_csv('data/theeuropeancommission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
